# do_legend<-TRUE
#DBF(US-UMB;RU-Fyo);ENF(US-NR1,FI-Hyy);MF(US-PFa,US-Syv)
plot_2groups(df_all,"FI-Hyy","temp_min_fluxnet2015","(degreeC)",TRUE)
##
# df<-df_all
# site<-"US-UMB"
# comp_var<-"temp_min_fluxnet2015"
#
# do_legend<-TRUE
#DBF(US-UMB;RU-Fyo);ENF(US-NR1,FI-Hyy);MF(US-PFa,US-Syv)
plot_2groups(df_all,"US-PFa","temp_min_fluxnet2015","(degreeC)",TRUE)
##
# df<-df_all
# site<-"US-UMB"
# comp_var<-"temp_min_fluxnet2015"
#
# do_legend<-TRUE
#DBF(US-UMB;RU-Fyo);ENF(US-NR1,FI-Hyy);MF(US-PFa,US-Syv)
plot_2groups(df_all,"US-Syv","temp_min_fluxnet2015","(degreeC)",TRUE)
##
# df<-df_all
# site<-"US-UMB"
# comp_var<-"temp_min_fluxnet2015"
#
# do_legend<-TRUE
#DBF(US-UMB;RU-Fyo);ENF(US-NR1,FI-Hyy);MF(US-PFa,US-Syv)
plot_2groups(df_all,"US-Syv","temp_min_fluxnet2015","(degreeC)",TRUE)
#############################################################################
#Aim:compare the variables in "event" and "non-event" site years after aligning the data-->specifically for Temp
library(dplyr)
#-------------------------------------------------------------------------
#(1)load the data that includes the "is_event" information
#-------------------------------------------------------------------------
#---------------------
#A.load the event_length data
#---------------------
load.path<-"./data/event_length/"
load(paste0(load.path,"df_events_length.RDA"))
#
tt<-df_events_all %>%
group_by(sitename) %>%
dplyr::summarise(Years=range(Year))
View(tt)
##---------------------------------------
#Aim: To each site, align different years with start of season and compare different
#years' environmental variables-->to see if if lower Tmin with more severe overestimation
##---------------------------------------
#-------------------------------------------------------------------------
##(1)selecting which several sites that are representative for the analysis:
#-------------------------------------------------------------------------
#dding the information of how many percentage of year with spring bias%
event.data.path<-"./data/event_length/"
load(file=paste0(event.data.path,"df_events_length.RDA"))
#first add a flag to indicate if the sites have overestimation or not;
df_events_all<-df_events_all %>%
mutate(event_flag=rep(NA,nrow(df_events_all)),
event_flag=ifelse(Over_days_length<20|is.na(Over_days_length),"no","yes"))
t1<-df_events_all %>%
group_by(sitename)%>%
dplyr::summarise(nyear=length(unique(Year)))
t2<-df_events_all%>%
filter(event_flag=="yes")%>%
group_by(sitename)%>%
dplyr::summarise(nyear_event=length(unique(Year)),
event_day_mean=mean(Over_days_length),
event_day_mean=round(event_day_mean,2))
t_merge<-left_join(t1,t2)
event_merge<-t_merge%>%
mutate(event_day_mean=ifelse(is.na(event_day_mean),0,event_day_mean))%>%
mutate(event_perc=nyear_event/nyear)%>%
mutate(event_perc=ifelse(is.na(event_perc),0,event_perc),
event_perc=round(event_perc,2))
#-->select several sites for further analysis:
#selection crition:1)sites with high and intermediate gpp overestimation;
#2)different PFT;(3)different continent:North America; Eurasia
#3)as many years data as possible
#DBF(US-UMB;RU-Fyo);ENF(US-NR1,FI-Hyy);MF(US-PFa,US-Syv)
#-------------------------------------------------------------------------
##(2).load the data
#-------------------------------------------------------------------------
load(paste0("./data/data_used/","ddf_align_data_from_sos.RDA"))
#put all the years data from each site together:
df_all<-rbind(df_len5_nonnorm$df_dday,df_len5_nonnorm$df_noevent_dday)
#-------------------------------------------------------------------------
##(3).making the plots
#-------------------------------------------------------------------------
#prepare the plot function:
plot_2groups<-function(df,site,comp_var,var_unit,do_legend){
# df<-df_all
# site<-"US-UMB"
# comp_var<-"temp_min_fluxnet2015"
# var_unit<-"(degreeC)"
# do_legend<-TRUE
#------------------------------------
#data tidy and select the relevant variables
#------------------------------------
df_sel<-df%>%
filter(sitename==site)
###
#selected the most relevant vars
#for the comparison,select the original variable if "do_norm"==FALSE,otherwise "do_norm"==TRUE
df.sel_temp<-df_sel[,c("sitename","Year","date","dday","gpp_res",comp_var)] #gpp_res:gpp_mod-gpp_obs
names(df.sel_temp)<-c("sitename","Year","date","dday","bias","comp_var")
##do the summary analysis to determine the orders of years that are overestimated:
summary.bias<-df.sel_temp %>%
filter(dday>=0)%>%
group_by(Year)%>%
dplyr::summarise(spring_bias=round(mean(bias,na.rm=T),3))
df.all<-left_join(df.sel_temp,summary.bias)
df.all$spring_bias<-as.factor(df.all$spring_bias)
#--------------------------------------
#plotting
#--------------------------------------
#y axis range:
ymin<-min(range(df.sel_temp$comp_var,na.rm = T))
ymax<-max(range(df.sel_temp$comp_var,na.rm = T))
#x axis range
# x_range_event<-range(df.event_sel$doy)
# x_range_nonevent<-range(df.nonevent_sel$doy)
#
####start to make the plots:
p_plot<-df.all%>%
ggplot(aes(x=dday,y=comp_var,col=spring_bias))+
geom_point(size=1)+
geom_smooth(se=FALSE,method = "lm")+
scale_color_viridis_d()+
ylab(paste0(comp_var," ",var_unit))+
xlab("gday")+
annotate("rect",xmin=0,xmax=70,ymin = -Inf,ymax = Inf,alpha=0.2)+
theme_classic()+
theme(legend.background = element_blank(),
legend.key.size = unit(2, 'lines'),
legend.title = element_text(size=24),
legend.text = element_text(size=18),
axis.title = element_text(size=20),
axis.text = element_text(size = 20))+
theme(legend.text.align = 0)+  #align the legend (all the letter start at the same positoin)
xlim(-60,70)  #add x range in 2021-09-25
#legend
if(do_legend==FALSE){
p_plot<-p_plot+
theme(legend.position = "none")
}
# print(p_plot)
#returun object
return(p_plot)
}
##
#DBF(US-UMB;RU-Fyo);ENF(US-NR1,FI-Hyy);MF(US-PFa,US-Syv)
sel_sites<-c("US-UMB","RU-Fyo","US-NR1","FI-Hyy","US-PFa","US-Syv")
plot_all<-c()
for (i in 1:length(sel_sites)) {
plot_all[[i]]<-plot_2groups(df_all,"US-Syv","temp_min_fluxnet2015","(degreeC)",TRUE)
}
##---------------------------------------
#Aim: To each site, align different years with start of season and compare different
#years' environmental variables-->to see if if lower Tmin with more severe overestimation
##---------------------------------------
library(ggplot2)
##
#DBF(US-UMB;RU-Fyo);ENF(US-NR1,FI-Hyy);MF(US-PFa,US-Syv)
sel_sites<-c("US-UMB","RU-Fyo","US-NR1","FI-Hyy","US-PFa","US-Syv")
plot_all<-c()
for (i in 1:length(sel_sites)) {
plot_all[[i]]<-plot_2groups(df_all,"US-Syv","temp_min_fluxnet2015","(degreeC)",TRUE)
}
plot_all[[1]]
#merge the plots:
library(ggpubr)
ggarrange(plot_all[[1]],plot_all[[2]],plot_all[[3]],
plot_all[[4]],plot_all[[5]],plot_all[[6]]
)
ggarrange(plot_all[[1]],plot_all[[2]],plot_all[[3]],
plot_all[[4]],plot_all[[5]],plot_all[[6]]
)
ggarrange(plot_all[[1]],plot_all[[2]],plot_all[[3]],
plot_all[[4]],plot_all[[5]],plot_all[[6]]
)
##
#DBF(US-UMB;RU-Fyo);ENF(US-NR1,FI-Hyy);MF(US-PFa,US-Syv)
sel_sites<-c("US-UMB","RU-Fyo","US-NR1","FI-Hyy","US-PFa","US-Syv")
plot_all<-c()
for (i in 1:length(sel_sites)) {
plot_temp<-plot_2groups(df_all,sel_sites[i],"temp_min_fluxnet2015","(degreeC)",TRUE)
plot_all[[i]]<-plot_temp+
ylab(expression("T"[min]*" (Â°C)"))+
annotate(geom = "text",x=40,y=-30,label=sel_sites[i],size=4)
}
#merge the plots:
library(ggpubr)
ggarrange(plot_all[[1]],plot_all[[2]],plot_all[[3]],
plot_all[[4]],plot_all[[5]],plot_all[[6]]
)
#save the plot:
save.path<-"./test/align_plot_diffyears_foreachsite/"
align.plot<-ggarrange(plot_all[[1]],plot_all[[2]],plot_all[[3]],
plot_all[[4]],plot_all[[5]],plot_all[[6]]
)
#save the plot:
save.path<-"./test/align_plot_diffyears_foreachsite/"
ggsave(align.plot,paste0(save.path,"align.plot_eachsite.pdf"),
width = 20,height = 12)
align.plot<-ggarrange(plot_all[[1]],plot_all[[2]],plot_all[[3]],
plot_all[[4]],plot_all[[5]],plot_all[[6]]
)
#save the plot:
save.path<-"./test/align_plot_diffyears_foreachsite/"
ggsave(align.plot,paste0(save.path,"align.plot_eachsite.pdf"),
width = 20,height = 12)
?ggsave
ggsave(paste0(save.path,"align.plot_eachsite.png"),align.plot,
width = 20,height = 12)
library(dplyr)
library(lubridate)
library(ggplot2)
library(tidyr)
library(lme4)
library(tidyverse)
# remotes::install_github("computationales/ingestr") #install the package
library(ingestr)
devtools::load_all("D:/Github/rbeni/")
library(rbeni)
#-------------------------
#(1)load the data and hardening funciton
#-------------------------
#####
#load the data uploaded by Koen
df_recent <- readRDS(paste0("./data-raw/raw_data/P_model_output/model_data.rds")) %>%
mutate(
year = format(date, "%Y")
) %>%
na.omit()
#load the data Beni sent me before:
df_old<-read.csv(file=paste0("./data-raw/raw_data/Data_sent_by_Beni/","ddf_fluxnet2015_pmodel_with_forcings_stocker19gmd.csv"))
df_old<-df_old %>%
mutate(date=lubridate::mdy(date),
year=lubridate::year(date)) %>%
na.omit(gpp_obs)
#load the temperature acclimation factor fT:
# source(paste0("./R/functions_in_model/model_hardening_byBeni_addbaseGDD_rev.R"))
source(paste0("./R/functions_in_model/newly_formulated_fun/model_fT_rev.R"))
#--------------------------------------------------------------
#(2) retreive the optimized parameter for the sites
#--------------------------------------------------------------
# set initial value
par <- c("tau"=5,"X0"=-10,"Smax"=5,"k"=1)
#
lower=c(1,-10,5,0)
upper=c(25,10,25,2)
# run model and compare to true values
# returns the RMSE
cost <- function(
data,
par
) {
scaling_factor <- data %>%
# group_by(sitename) %>%
do({
scaling_factor <- f_Ts_rev(
.,
par
)
data.frame(
sitename = .$sitename,
date = .$date,
scaling_factor = scaling_factor
)
})
df <- left_join(data, scaling_factor)
#rmse
# rmse <- sqrt(
#   sum(
#     (df$gpp - df$gpp_mod * df$scaling_factor)^2)
#   )/nrow(df)
#mse:mean square error
mse<-mean((df$gpp - df$gpp_mod * df$scaling_factor)^2,na.rm=T)
return(mse)
}
#--------------------------------------------------------------
#(3) parameters optimization (for site-level, PFT-level, and All-sites level)
#--------------------------------------------------------------
#a.adding the PFTs information for site-->load the modis data-->tidy from Beni
load(paste0("./data-raw/raw_data/sites_info/","Pre_selected_sites_info.RDA"))
sites.info<-df_sites_sel
#
df_merge<-df_recent %>%
left_join(
sites.info,
by = "sitename"
)
##adding day of the year
df_merge$doy<-yday(df_merge$date)
#main Clim-PFTs
df_merge$Clim_PFTs<-paste0(df_merge$koeppen_code,"-",df_merge$classid)
Clim.PFTs<-sort(unique(df_merge$Clim_PFTs))
#------------------------------------------
#b.normalized the GPP-->for each site,
#normalized the "gpp" and "gpp_mod" through their 90 percentiles
#------------------------------------------
#I should use the same value to normlize the gpp and gpp_mod:
gpp_P95<-df_merge %>%
group_by(sitename) %>%
dplyr::summarise(gpp_norm_p95=quantile(c(gpp,gpp_mod),0.95,na.rm=T))
#
df_merge.new<-left_join(df_merge,gpp_P95,by="sitename")
df_merge.new<-df_merge.new %>%
mutate(gpp=gpp/gpp_norm_p95,gpp_mod=gpp_mod/gpp_norm_p95)
# need to remove the sites that do not used in this analysis:
rm.sites<-c("BE-Bra","CA-SF1","CA-SF2","FI-Sod","US-Wi4")
df_merge_new<-df_merge.new %>%
filter(sitename!=rm.sites[1] & sitename!=rm.sites[2]&sitename!=rm.sites[3]&sitename!=rm.sites[4]&sitename!=rm.sites[5])
#--------------------------------------------------------------
#(4) compare the gpp_obs, ori modelled gpp, and gpp modelled
# using optimated parameters calibrated on different scale(for site-level, PFT-level, and All-sites level)
#--------------------------------------------------------------
#---a.load the optimized parameters from different scales
# site-level
load(paste0("./data/model_parameters/parameters_MAE_newfT/","optim_par_run5000_eachsite.rds"))
#PFT-level
# load(paste0("./data/model_parameters/parameters_MAE_newfT/","optim_par_run5000_PFTs.rds"))
#with updated parameter
load(paste0("./data/model_parameters/parameters_MAE_newfT/","optim_par_run5000_PFTs_with_newMF_paras.rds"))
#All-sites level
load(paste0("./data/model_parameters/parameters_MAE_newfT/","optim_par_run5000_allsites.rds"))
#---b.retrieve the stress factor(calibration factor) for each scale-level
#--------
#for optimized paramater from site-level calibration:
#--------
sel_sites<-unique(df_merge_new$sitename)
df_final_sitelevel<-c()
for (i in 1:length(sel_sites)) {
df_sel<-df_merge_new %>%
dplyr::filter(sitename==sel_sites[i])
scaling_factors <- df_sel %>%
# group_by(sitename, year) %>%
do({
scaling_factor <- f_Ts_rev(.,par_mutisites[[i]])
data.frame(
sitename = .$sitename,
date = .$date,
scaling_factor_optim = scaling_factor
)
})
df_sel <- left_join(df_sel, scaling_factors)
#merge different sites:
df_final_sitelevel<-rbind(df_final_sitelevel,df_sel)
}
#--------
#for optimized paramater from PFT-level calibration:
#--------
PFTs<-unique(df_merge_new$classid)
df_final_PFTlevel<-c()
for (i in 1:length(PFTs)) {
df_sel<-df_merge_new %>%
dplyr::filter(classid==PFTs[i])
scaling_factors <- df_sel %>%
# group_by(sitename, year) %>%
do({
scaling_factor <- f_Ts_rev(.,par_PFTs[[i]])
data.frame(
sitename = .$sitename,
date = .$date,
scaling_factor_optim = scaling_factor
)
})
df_sel_new <- left_join(df_sel, scaling_factors)
#merge different sites:
df_final_PFTlevel<-rbind(df_final_PFTlevel,df_sel_new)
}
#--------
#for optimized paramater from All sites-level calibration:
#--------
df_final_Allsiteslevel<-c()
#
df_sel<-df_merge_new
scaling_factors <- df_sel %>%
# group_by(sitename, year) %>%
do({
scaling_factor <- f_Ts_rev(.,par_allsites)
data.frame(
sitename = .$sitename,
date = .$date,
scaling_factor_optim = scaling_factor
)
})
df_sel_new <- left_join(df_sel, scaling_factors)
#merge different sites:
df_final_Allsiteslevel<-df_sel_new
#-----------------------------
#---c.need to back-convert the normalized gpp to gpp
#-----------------------------
#for site-level:
df_final_sitelevel_new<-df_final_sitelevel %>%
mutate(gpp=gpp*gpp_norm_p95,
gpp_mod=gpp_mod*gpp_norm_p95,
year=year(date))
df_merge_sitelevel<-left_join(df_final_sitelevel_new,df_old,by = c("sitename", "date", "year")) %>%
mutate(gpp_obs_recent=gpp,
gpp_obs_old=gpp_obs,
gpp_mod_FULL_ori=gpp_mod_FULL,
gpp_mod_recent_ori=gpp_mod,
gpp_mod_recent_optim=gpp_mod*scaling_factor_optim,
gpp=NULL,
gpp_obs=NULL,
gpp_mod=NULL)
#for PFT-level:
df_final_PFTlevel_new<-df_final_PFTlevel %>%
mutate(gpp=gpp*gpp_norm_p95,
gpp_mod=gpp_mod*gpp_norm_p95,
year=year(date))
df_merge_PFTlevel<-left_join(df_final_PFTlevel_new,df_old,by = c("sitename", "date", "year")) %>%
mutate(gpp_obs_recent=gpp,
gpp_obs_old=gpp_obs,
gpp_mod_FULL_ori=gpp_mod_FULL,
gpp_mod_recent_ori=gpp_mod,
gpp_mod_recent_optim=gpp_mod*scaling_factor_optim,
gpp=NULL,
gpp_obs=NULL,
gpp_mod=NULL)
#for Allsites-level:
df_final_Allsiteslevel_new<-df_final_Allsiteslevel %>%
mutate(gpp=gpp*gpp_norm_p95,
gpp_mod=gpp_mod*gpp_norm_p95,
year=year(date))
df_merge_Allsiteslevel<-left_join(df_final_Allsiteslevel_new,df_old,by = c("sitename", "date", "year")) %>%
mutate(gpp_obs_recent=gpp,
gpp_obs_old=gpp_obs,
gpp_mod_FULL_ori=gpp_mod_FULL,
gpp_mod_recent_ori=gpp_mod,
gpp_mod_recent_optim=gpp_mod*scaling_factor_optim,
gpp=NULL,
gpp_obs=NULL,
gpp_mod=NULL)
names(df_merge_PFTlevel)
##general assessment: how much bias was reduced(during the green-up period) for
##original p-model and adjusted p-model(using PFT-specific parameters):
#
df_modobs_PFTlevel<-df_merge_PFTlevel%>%
select(sitename,date,gpp_obs_recent,gpp_mod_FULL_ori,gpp_mod_recent_ori,gpp_mod_recent_optim)%>%
mutate(gpp_obs=gpp_obs_recent,
gpp_mod_old_ori=gpp_mod_FULL_ori,             #gpp_mod_old_ori-->corrsponds to Stocker et al., 2022
gpp_mod_recent_ori=gpp_mod_recent_ori,        #gpp_mod_recent_ori-->updated GPP from Beni
gpp_mod_recent_optim=gpp_mod_recent_optim) %>%#gpp_mod_recent_optim-->updated GPP calibrated with paras
mutate(gpp_obs_recent=NULL,
gpp_mod_FULL_ori=NULL)
rm(df_modobs_PFTlevel)
##general assessment: how much bias was reduced(during the green-up period) for
##original p-model and adjusted p-model(using PFT-specific parameters):
#
df_modobs_comp<-df_merge_PFTlevel%>%
select(sitename,date,gpp_obs_recent,gpp_mod_FULL_ori,gpp_mod_recent_ori,gpp_mod_recent_optim)%>%
mutate(gpp_obs=gpp_obs_recent,
gpp_mod_old_ori=gpp_mod_FULL_ori,             #gpp_mod_old_ori-->corrsponds to Stocker et al., 2022
gpp_mod_recent_ori=gpp_mod_recent_ori,        #gpp_mod_recent_ori-->updated GPP from Beni
gpp_mod_recent_optim=gpp_mod_recent_optim) %>%#gpp_mod_recent_optim-->updated GPP calibrated with paras
mutate(gpp_obs_recent=NULL,
gpp_mod_FULL_ori=NULL)
names(df_modobs_comp)
names(df_merge_PFTlevel)
paste0(pheno.path,"df_events_length.RDA")
##general assessment: how much bias was reduced(during the green-up period) for
##original p-model and adjusted p-model(using PFT-specific parameters):
#load the green-up data:
pheno.path<-"./data/event_length/"
pheno.path,"df_events_length.RDA")
paste0(pheno.path,"df_events_length.RDA")
load(paste0(pheno.path,"df_events_length.RDA"))
View(df_events_all)
names(df_events_all)
names(df_merge_PFTlevel)
df_pheno<-df_events_all %>%
select(sitename,Year,sos,peak)%>%
mutate(year=Year,Year=NULL)
df_merge_PFTlevel<-left_join(df_merge_PFTlevel,df_pheno)
View(df_merge_PFTlevel)
names(df_merge_PFTlevel)
t<-df_merge_PFTlevel%>%
filter(doy>=sos & doy<=peak)
## update in Nov,2022
df_modobs_comp<-df_merge_PFTlevel%>%
filter(doy>=sos & doy<=peak) %>% ##only select the data during the growing season
select(sitename,date,year,gpp_obs_recent,gpp_mod_FULL_ori,gpp_mod_recent_ori,gpp_mod_recent_optim)%>%
mutate(gpp_obs=gpp_obs_recent,
gpp_mod_old_ori=gpp_mod_FULL_ori,             #gpp_mod_old_ori-->corrsponds to Stocker et al., 2022
gpp_mod_recent_ori=gpp_mod_recent_ori,        #gpp_mod_recent_ori-->updated GPP from Beni
gpp_mod_recent_optim=gpp_mod_recent_optim) %>%#gpp_mod_recent_optim-->updated GPP calibrated with paras
mutate(gpp_obs_recent=NULL,
gpp_mod_FULL_ori=NULL)
View(df_modobs_comp)
names(df_modobs_comp)
##how much bias is reduced for the model with optimized parameter compaerd to old original p-model:
library(sirad)
?modeval
modeval(df_modobs_comp$gpp_mod_old_ori,df_modobs_comp$gpp_obs,
stat = c("MAE","RMAE","RMSE","RRMSE"))
unlist(modeval(df_modobs_comp$gpp_mod_old_ori,df_modobs_comp$gpp_obs,
stat = c("MAE","RMAE","RMSE","RRMSE")))
unlist(modeval(df_modobs_comp$gpp_mod_recent_optim,df_modobs_comp$gpp_obs,
stat = c("MAE","RMAE","RMSE","RRMSE")))
##how much bias is reduced for the model with optimized parameter compaerd to old original p-model:
library(sirad)
MAE_Pmodel_ori<-unlist(modeval(df_modobs_comp$gpp_mod_old_ori,df_modobs_comp$gpp_obs,
stat = c("MAE","RMAE","RMSE","RRMSE")))
MAE_Pmodel_optim<-unlist(modeval(df_modobs_comp$gpp_mod_recent_optim,df_modobs_comp$gpp_obs,
stat = c("MAE","RMAE","RMSE","RRMSE")))
MAE_Pmodel_ori
MAE_Pmodel_optim
MAE_Pmodel_optim[1]
#reduced bias:
c(MAE_Pmodel_ori[1]-MAE_Pmodel_optim[1])/MAE_Pmodel_ori[1]
#reduced bias:
c(MAE_Pmodel_ori[1]-MAE_Pmodel_optim[1])/MAE_Pmodel_optim[1]
