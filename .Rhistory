geom_point()+
geom_hline(yintercept = 0,col="blue",lty=2)+
facet_wrap(flag~.,nrow = 2)
#plot 1:general distribution of Tsoil
df.merge_soil %>%
group_by(flag)%>%
ggplot(aes(x=date,y=TS_1_fluxnet2015,col=sitename))+
geom_point()+
geom_hline(yintercept = 0,col="blue",lty=2)+
facet_wrap(flag~.,nrow = 2)
#plot 2:
df.merge_soil %>%
group_by(flag)%>%
ggplot(aes(x=date,y=TS_1_fluxnet2015,col=month))+
geom_point()+
geom_hline(yintercept = 0,col="blue",lty=2)+
scale_color_viridis_b()+
facet_wrap(flag~.,nrow = 2)
#plot 1:general distribution of Tsoil
df.merge_soil %>%
group_by(flag)%>%
ggplot(aes(x=date,y=TS_1_fluxnet2015,col=sitename))+
geom_point()+
geom_hline(yintercept = 0,col="blue",lty=2)+
facet_wrap(flag~.,nrow = 2)
library(plotly)
install.packages("plotly")
library(plotly)
#plot 1:general distribution of Tsoil
plot1<-df.merge_soil %>%
group_by(flag)%>%
ggplot(aes(x=date,y=TS_1_fluxnet2015,col=sitename))+
geom_point()+
geom_hline(yintercept = 0,col="blue",lty=2)+
facet_wrap(flag~.,nrow = 2)
ggplotly(plot1)
install.packages("htmlwidgets")
library(htmlwidgets)
#plot 1:general distribution of Tsoil
plot1<-df.merge_soil %>%
group_by(flag)%>%
ggplot(aes(x=date,y=TS_1_fluxnet2015,col=sitename))+
geom_point()+
geom_hline(yintercept = 0,col="blue",lty=2)+
facet_wrap(flag~.,nrow = 2)
plot1_dynamic<-ggplotly(plot1)
library(plotly) #plot interactive plots
library(htmlwidgets) #save the plot to html files
#############################################################################
#Aim:compare the variables in "event" and "non-event" site years after aligning the data-->specifically for Temp
library(dplyr)
#-------------------------------------------------------------------------
#(1)load the data that includes the "is_event" information
#-------------------------------------------------------------------------
#---------------------
#A.load the event_length data
#---------------------
load.path<-"./data/event_length/"
load(paste0(load.path,"df_events_length.RDA"))
#
tt<-df_events_all %>%
group_by(sitename) %>%
dplyr::summarise(Years=range(Year))
# a function to separate out the site-year that event_length higher than some thresholds(e.g. 30 days)
sep_siteyears<-function(df,sep_var,sep_thrs){
# df<-df_events_all
# sep_var<-"Over_days_length"
# sep_thrs<-30
#
df_sep<-df[df[,sep_var]>sep_thrs & !is.na(df[,sep_var]),]
pos_sep<-as.vector(which(df[,sep_var]>sep_thrs))
#as some site the sep_var==NA, hence using this way to separate the data
pos_left<-setdiff(as.vector(1:length(df[,sep_var])),pos_sep)
df_left<-df[pos_left,]
df_new<-list(df_sep,df_left)
names(df_new)<-c("event_siteyears","noevent_siteyears")
return(df_new)
}
##separate the site-year when the over_days_length > 20 days
df.sep20<-sep_siteyears(df_events_all,"Over_days_length",20)
#---------------------
#B.load the data
#---------------------
load.path<-"./data/data_used/"
#from new method:
load(paste0(load.path,"ddf_labeled_norm_trs_newmethod_all_overestimation_Fluxnet2015_sites.RDA"))
df_all_sites<-ddf_labeled;rm(ddf_labeled)
df_norm_all<-df_all_sites
#-------------------------------------------------------------------------
#(2)start to align the data according to Beni's functions of "align_events" and "get_consecutive"
#-------------------------------------------------------------------------
#---------------------------------
#source the functions to detect the consecutive events
#---------------------------------
fun.path<-"./R/functions_from_beni/"
#source get_consecutive.R
source(paste0(fun.path,"get_consecutive.R"))
#source get_consecutive_greenup.R
source(paste0(fun.path,"get_consecutive_greenup.R"))
#source align_event.R -->for event site years
source(paste0(fun.path,"align_events_df.R"))
#source align_nonevent.R -->for non-event site years
source(paste0(fun.path,"align_nonevents_df.R"))
sep_siteyears_data<-function(df.data,dovars,df.sep,leng_threshold,before,after,nbins,do_norm){
# df.data<-df_norm_all
# dovars<-c("gpp_obs")
# df.sep<-df.sep20
# leng_threshold<-5
# before=30
# after=0
# nbins=10
# do_norm=TRUE
#-------------------------------------------------------------------------
#A.separating the datasets:"event" and "nonevent" site years
#-------------------------------------------------------------------------
N<-nrow(df.sep$event_siteyears)
df_events.years<-c()    #target
df_nonevents.years<-c() #target
#for is.event years
pos.isevent<-c()
for(i in 1:N){
df.temp<-subset(df.data,sitename==df.sep$event_siteyears$sitename[i] & Year==df.sep$event_siteyears$Year[i])
pos.temp<-c(which(df.data$sitename==df.temp$sitename & df.data$Year==df.temp$Year))
df_events.years<-rbind(df_events.years,df.temp)
pos.isevent<-c(pos.isevent,pos.temp)
}
#for no is.event years
pos.non_isevent<-setdiff(c(1:nrow(df.data)),pos.isevent)
df_nonevents.years<-df.data[pos.non_isevent,] #target
#---------------------------------
#B.start to align different events(for event site years) and green-up(for non-event site years) in each site-year
#---------------------------------
# df<-df_norm_all
# leng_threshold<-20
# before=30
# after=30
# nbins=10
# do_norm=FALSE
#function format
# align_events <- function( df, leng_threshold, before, after, nbins, do_norm=FALSE )
#at this stage-->bins do not used(now set default value as 10)
#-------------
#set different length_threshold-->5 days(consecutive 5 days overestimation will be an event)
#-------------
#and select the 30 days before the events
df_len_events<-align_events(df_events.years,dovars,leng_threshold = leng_threshold,before = before,after = after,
nbins = nbins,do_norm = do_norm)
print("ok")
df_len_nonevents<-align_nonevents(df_nonevents.years,dovars,leng_threshold = leng_threshold,before = before,after = after,
nbins = nbins,do_norm = do_norm)
#---------------------------------
#C.separate the df_event and df_nonevent;
#---------------------------------
df_all<-c()
#for event site years
df_dday<-df_len_events$df_dday
#for non_event site years, take the doy belongs to green-up period:
df_noevent_dday<-df_len_nonevents$df_dday
#
df_all<-list(df_dday=df_dday,df_noevent_dday=df_noevent_dday)
return(df_all)
}
#!!important step:leng_threshold=5-->merge the events(consecutive days are over 5 days)
#do_vars-->the variables that are going to be processed:
names(df_norm_all)
do_vars<-c("gpp_obs","fapar_itpl","fapar_spl",paste0(c("ppfd","PPFD_IN_fullday_mean","temp_day","temp_min","temp_max",
"vpd_day","prec","patm","SW_IN","ws",paste0("TS_",1:7),paste0("SWC_",1:5)),"_fluxnet2015"),
"gcc_90","rcc_90")
#set the before events days from 30 days to 60 days
df_len5_nonnorm<-sep_siteyears_data(df_norm_all,do_vars,df.sep20,5,60,0,10,FALSE)
#-------------------------------------------------------------------------
#(3)calculating some important variables like LUE...
#-------------------------------------------------------------------------
#--------------------------
#calculating more variables
#--------------------------
#1)LUE=GPP/fAPAR*ppfd
#unit-->GPP: umol m-2 s-1; ppdf-->umol m-2 s-1; fAPRA: unitless
#2)GRVI=(gcc-rcc)/c(gcc+rcc)
#3)albedo:alpha_SW<-SW_OUT/SW_IN; alpha_ppdf<-PPFD_IN/PPFD_OUT
#4)approximated fAPARchl=EVI*factor(factor = 1)
#check the data aviablity for each variable:
library(visdat)
#for overestimated sites and years
pos_TS<-grep("TS_",names(df_len5_nonnorm$df_dday))
pos_SWC<-grep("SWC_",names(df_len5_nonnorm$df_dday))
visdat::vis_miss(df_len5_nonnorm$df_dday[,pos_TS], warn_large_data = FALSE)
visdat::vis_miss(df_len5_nonnorm$df_dday[,pos_SWC], warn_large_data = FALSE)
#for non-overestimated sites and years
pos_TS<-grep("TS_",names(df_len5_nonnorm$df_noevent_dday))
pos_SWC<-grep("SWC_",names(df_len5_nonnorm$df_noevent_dday))
visdat::vis_miss(df_len5_nonnorm$df_noevent_dday[,pos_TS], warn_large_data = FALSE)
visdat::vis_miss(df_len5_nonnorm$df_noevent_dday[,pos_SWC], warn_large_data = FALSE)
for(i in 1:length(df_len5_nonnorm)){
#
df_proc<-df_len5_nonnorm[[i]]
df_proc$LUE<-df_proc$gpp_obs/c(df_proc$fapar_itpl*df_proc$PPFD_IN_fullday_mean_fluxnet2015)
df_proc$GRVI<-c(df_proc$gcc_90-df_proc$rcc_90)/c(df_proc$gcc_90+df_proc$rcc_90)
df_proc$alpha_SW<-df_proc$SW_OUT_fullday_mean_fluxnet2015/df_proc$SW_IN_fullday_mean_fluxnet2015
df_proc$alpha_PPFD<-df_proc$PPFD_OUT_fullday_mean_fluxnet2015/df_proc$PPFD_IN_fullday_mean_fluxnet2015
df_proc$fAPAR_chl<-df_proc$evi*1
##adding the mean values of Tsoil and SWC of first 3 layers(updated in 2022,Oct):
df_proc$TS_top3_fluxnet2015<-rowMeans(df_proc[,c("TS_1_fluxnet2015",
"TS_2_fluxnet2015","TS_3_fluxnet2015")],na.rm = T)
df_proc$SWC_top3_fluxnet2015<-rowMeans(df_proc[,c("SWC_1_fluxnet2015",
"SWC_2_fluxnet2015","SWC_3_fluxnet2015")],na.rm = T)
#assign value back:
df_len5_nonnorm[[i]]<-df_proc
}
#a1.checking the general distribution of Tsoil
# par(mfrow=c(2,1),fig=c(0,1,0.4,1))
# plot(df_len5_nonnorm$df_dday$date,df_len5_nonnorm$df_dday$TS_1_fluxnet2015,
#      xlab = "",ylab="SY_PSB Tsoil(degC)",xaxt="n")
# abline(h=0,lty=2,col="blue")
# par(fig=c(0,1,0,0.6),new=T)
# plot(df_len5_nonnorm$df_noevent_dday$date,df_len5_nonnorm$df_noevent_dday$TS_1_fluxnet2015,
#      xlab = "Date",ylab="SY_ASB Tsoil(degC)")
# abline(h=0,lty=2,col="blue")
#a2.check the missing Tsoil sites
#take out the TS from two classes and aggregate them:
df1<-df_len5_nonnorm[["df_dday"]] %>%
dplyr::select(sitename,date,date,greenup,is_event,TS_1_fluxnet2015,SWC_1_fluxnet2015)%>%
mutate(month=month(date))%>%
mutate(flag="SY_PSB")
df2<-df_len5_nonnorm[["df_noevent_dday"]] %>%
dplyr::select(sitename,date,date,greenup,is_event,TS_1_fluxnet2015,SWC_1_fluxnet2015)%>%
mutate(month=month(date))%>%
mutate(flag="SY_ASB")
#check the how many site the data are available:
#for overestimated sites:
N1<-length(unique(df1[!is.na(df1$TS_1_fluxnet2015),]$sitename))
N2<-length(unique(df1$sitename))
print(c(N1,N2))
#for non-overestimated sites:
N1<-length(unique(df2[!is.na(df2$TS_1_fluxnet2015),]$sitename))
N2<-length(unique(df2$sitename))
print(c(N1,N2))
#-------------------------------------------------------------------------
#--------------------------
#calculating more variables
#--------------------------
#1)LUE=GPP/fAPAR*ppfd
#unit-->GPP: umol m-2 s-1; ppdf-->umol m-2 s-1; fAPRA: unitless
#2)GRVI=(gcc-rcc)/c(gcc+rcc)
#3)albedo:alpha_SW<-SW_OUT/SW_IN; alpha_ppdf<-PPFD_IN/PPFD_OUT
#4)approximated fAPARchl=EVI*factor(factor = 1)
#check the data aviablity for each variable:
library(lubridate)
library(visdat)
#for overestimated sites and years
pos_TS<-grep("TS_",names(df_len5_nonnorm$df_dday))
pos_SWC<-grep("SWC_",names(df_len5_nonnorm$df_dday))
visdat::vis_miss(df_len5_nonnorm$df_dday[,pos_TS], warn_large_data = FALSE)
visdat::vis_miss(df_len5_nonnorm$df_dday[,pos_SWC], warn_large_data = FALSE)
#for non-overestimated sites and years
pos_TS<-grep("TS_",names(df_len5_nonnorm$df_noevent_dday))
pos_SWC<-grep("SWC_",names(df_len5_nonnorm$df_noevent_dday))
visdat::vis_miss(df_len5_nonnorm$df_noevent_dday[,pos_TS], warn_large_data = FALSE)
visdat::vis_miss(df_len5_nonnorm$df_noevent_dday[,pos_SWC], warn_large_data = FALSE)
for(i in 1:length(df_len5_nonnorm)){
#
df_proc<-df_len5_nonnorm[[i]]
df_proc$LUE<-df_proc$gpp_obs/c(df_proc$fapar_itpl*df_proc$PPFD_IN_fullday_mean_fluxnet2015)
df_proc$GRVI<-c(df_proc$gcc_90-df_proc$rcc_90)/c(df_proc$gcc_90+df_proc$rcc_90)
df_proc$alpha_SW<-df_proc$SW_OUT_fullday_mean_fluxnet2015/df_proc$SW_IN_fullday_mean_fluxnet2015
df_proc$alpha_PPFD<-df_proc$PPFD_OUT_fullday_mean_fluxnet2015/df_proc$PPFD_IN_fullday_mean_fluxnet2015
df_proc$fAPAR_chl<-df_proc$evi*1
##adding the mean values of Tsoil and SWC of first 3 layers(updated in 2022,Oct):
df_proc$TS_top3_fluxnet2015<-rowMeans(df_proc[,c("TS_1_fluxnet2015",
"TS_2_fluxnet2015","TS_3_fluxnet2015")],na.rm = T)
df_proc$SWC_top3_fluxnet2015<-rowMeans(df_proc[,c("SWC_1_fluxnet2015",
"SWC_2_fluxnet2015","SWC_3_fluxnet2015")],na.rm = T)
#assign value back:
df_len5_nonnorm[[i]]<-df_proc
}
# par(mfrow=c(2,1),fig=c(0,1,0.4,1))
# plot(df_len5_nonnorm$df_dday$date,df_len5_nonnorm$df_dday$TS_1_fluxnet2015,
#      xlab = "",ylab="SY_PSB Tsoil(degC)",xaxt="n")
# abline(h=0,lty=2,col="blue")
# par(fig=c(0,1,0,0.6),new=T)
# plot(df_len5_nonnorm$df_noevent_dday$date,df_len5_nonnorm$df_noevent_dday$TS_1_fluxnet2015,
#      xlab = "Date",ylab="SY_ASB Tsoil(degC)")
# abline(h=0,lty=2,col="blue")
#a2.check the missing Tsoil sites
#take out the TS from two classes and aggregate them:
df1<-df_len5_nonnorm[["df_dday"]] %>%
dplyr::select(sitename,date,date,greenup,is_event,TS_1_fluxnet2015,SWC_1_fluxnet2015)%>%
mutate(month=month(date))%>%
mutate(flag="SY_PSB")
df2<-df_len5_nonnorm[["df_noevent_dday"]] %>%
dplyr::select(sitename,date,date,greenup,is_event,TS_1_fluxnet2015,SWC_1_fluxnet2015)%>%
mutate(month=month(date))%>%
mutate(flag="SY_ASB")
#check the how many site the data are available:
#for overestimated sites:
N1<-length(unique(df1[!is.na(df1$TS_1_fluxnet2015),]$sitename))
N2<-length(unique(df1$sitename))
print(c(N1,N2))
#for non-overestimated sites:
N1<-length(unique(df2[!is.na(df2$TS_1_fluxnet2015),]$sitename))
N2<-length(unique(df2$sitename))
print(c(N1,N2))
#a3.merge df1 and df2 and plotting
df.merge_soil<-rbind(df1,df2)
library(plotly) #plot interactive plots
library(htmlwidgets) #save the plot to html files
#plot 1:general distribution of Tsoil
plot1<-df.merge_soil %>%
group_by(flag)%>%
ggplot(aes(x=date,y=TS_1_fluxnet2015,col=sitename))+
geom_point()+
geom_hline(yintercept = 0,col="blue",lty=2)+
facet_wrap(flag~.,nrow = 2)
plot1_dynamic<-ggplotly(plot1)
#save the plot
save.path<-"./test/check_reason_Tsoil_never_below_0degree/Figures/"
#save the plot
save.path<-"./test/check_reason_Tsoil_never_below_0degree/Figures/"
htmlwidgets::saveWidget(
widget = , #the plotly object
file = paste0("Tsoil_general_distributon.html"), #the path & file name
selfcontained = TRUE #creates a single html file
)
#save the plot
save.path<-"./test/check_reason_Tsoil_never_below_0degree/Figures/"
htmlwidgets::saveWidget(
widget = plot1_dynamic, #the plotly object
file = paste0("Tsoil_general_distributon.html"), #the path & file name
selfcontained = TRUE #creates a single html file
)
getwd()
#save the plot
save.path<-"./test/check_reason_Tsoil_never_below_0degree/Figures/"
htmlwidgets::saveWidget(
widget = plot1_dynamic, #the plotly object
file = paste0("Tsoil_general_distributon.html"), #the path & file name
selfcontained = TRUE #creates a single html file
)
#save the plot
save.path<-"./test/check_reason_Tsoil_never_below_0degree/Figures/"
htmlwidgets::saveWidget(
widget = plot1_dynamic, #the plotly object
file = paste0(save.path,"Tsoil_general_distributon.html"), #the path & file name
selfcontained = TRUE #creates a single html file
)
df.merge_soil %>%
group_by(flag)%>%
ggplot(aes(x=date,y=TS_1_fluxnet2015,col=month))+
geom_point()+
geom_hline(yintercept = 0,col="blue",lty=2)+
scale_color_viridis_b()+
facet_wrap(flag~.,nrow = 2)
names(df.merge_soil)
df.merge_soil %>%
mutate(doy=yday(date))%>%
group_by(flag)%>%
ggplot(aes(x=doy,y=TS_1_fluxnet2015))+
geom_point()
df.merge_soil %>%
mutate(doy=yday(date))%>%
group_by(flag)%>%
ggplot(aes(x=doy,y=TS_1_fluxnet2015,col=sitename))+
geom_point()+
geom_hline(yintercept = 0,col="blue",lty=2)+
scale_color_viridis_b()+
facet_wrap(flag~.,nrow = 2)
df.merge_soil %>%
mutate(doy=yday(date))%>%
group_by(flag)%>%
ggplot(aes(x=doy,y=TS_1_fluxnet2015,col=sitename))+
geom_point()
df.merge_soil %>%
mutate(doy=yday(date))%>%
group_by(flag,sitename)%>%
ggplot(aes(x=doy,y=TS_1_fluxnet2015,col=sitename))+
geom_point()
df.merge_soil %>%
mutate(doy=yday(date))%>%
group_by(flag,sitename)%>%
ggplot(aes(x=doy,y=TS_1_fluxnet2015,col=sitename))+
geom_point()+
geom_hline(yintercept = 0,col="blue",lty=2)+
scale_color_viridis_b()+
facet_wrap(flag~sitename,nrow = 2)
df.merge_soil %>%
mutate(doy=yday(date))%>%
group_by(sitename)%>%
ggplot(aes(x=doy,y=TS_1_fluxnet2015,col=sitename))+
geom_point()+
geom_hline(yintercept = 0,col="blue",lty=2)+
annotate(geom="text",x=20,y=5,label=flag)+
scale_color_viridis_b()+
facet_wrap(flag~sitename,nrow = 2)
df.merge_soil %>%
mutate(doy=yday(date))%>%
group_by(sitename)%>%
ggplot(aes(x=doy,y=TS_1_fluxnet2015,col=flag))+
geom_point()+
geom_hline(yintercept = 0,col="blue",lty=2)+
# annotate(geom="text",x=20,y=5)+
scale_color_viridis_b()+
facet_wrap(flag~sitename,nrow = 2)
df.merge_soil %>%
mutate(doy=yday(date))%>%
group_by(sitename)%>%
ggplot(aes(x=doy,y=TS_1_fluxnet2015,col=flag))+
geom_point()+
geom_hline(yintercept = 0,col="blue",lty=2)+
# annotate(geom="text",x=20,y=5)+
scale_color_viridis_b()+
facet_wrap(.~sitename,nrow = 2)
df.merge_soil %>%
mutate(doy=yday(date))%>%
group_by(sitename)%>%
ggplot(aes(x=doy,y=TS_1_fluxnet2015,col=flag))+
geom_point()
df.merge_soil %>%
mutate(doy=yday(date))%>%
group_by(sitename)%>%
ggplot(aes(x=doy,y=TS_1_fluxnet2015,col=flag))+
geom_point()+
geom_hline(yintercept = 0,col="blue",lty=2)+
# scale_color_viridis_b()+
facet_wrap(.~sitename,nrow = 2)
plot1
#save the plot
# save.path<-"./test/check_reason_Tsoil_never_below_0degree/Figures/"
# htmlwidgets::saveWidget(
#   widget = plot1_dynamic, #the plotly object
#   file = paste0(save.path,"Tsoil_general_distributon.html"), #the path & file name
#   selfcontained = TRUE #creates a single html file
# )
#plot 2:
plot2<-df.merge_soil %>%
mutate(doy=yday(date))%>%
group_by(sitename)%>%
ggplot(aes(x=doy,y=TS_1_fluxnet2015,col=flag))+
geom_point()+
geom_hline(yintercept = 0,col="blue",lty=2)+
# scale_color_viridis_b()+
facet_wrap(.~sitename,nrow = 2)
plot2_dynamic<-ggplotly(plot2)
#save the plot
# save.path<-"./test/check_reason_Tsoil_never_below_0degree/Figures/"
# htmlwidgets::saveWidget(
#   widget = plot1_dynamic, #the plotly object
#   file = paste0(save.path,"Tsoil_general_distributon.html"), #the path & file name
#   selfcontained = TRUE #creates a single html file
# )
#plot 2:
plot2<-df.merge_soil %>%
mutate(doy=yday(date))%>%
group_by(sitename)%>%
ggplot(aes(x=doy,y=TS_1_fluxnet2015,col=flag))+
geom_point()+
geom_hline(yintercept = 0,col="blue",lty=2)+
# scale_color_viridis_b()+
facet_wrap(.~sitename,nrow = 2)
plot2_dynamic<-ggplotly(plot2)
htmlwidgets::saveWidget(
widget = plot2_dynamic, #the plotly object
file = paste0(save.path,"Tsoil_vary_doy.html"), #the path & file name
selfcontained = TRUE #creates a single html file
)
htmlwidgets::saveWidget(
widget = plot2_dynamic, #the plotly object
file = paste0(save.path,"Tsoil_vary_doy_eachsite.html"), #the path & file name
selfcontained = TRUE #creates a single html file
)
print(c(N1,N2))
print(c(N1,N2))
#check the how many site the data are available:
#for overestimated sites:
N1<-length(unique(df1[!is.na(df1$TS_1_fluxnet2015),]$sitename))
N2<-length(unique(df1$sitename))
print(c(N1,N2))
#for non-overestimated sites:
N1<-length(unique(df2[!is.na(df2$TS_1_fluxnet2015),]$sitename))
N2<-length(unique(df2$sitename))
print(c(N1,N2))
df.merge_soil %>%
mutate(doy=yday(date))%>%
group_by(sitename)%>%
ggplot(aes(x=doy,y=TS_1_fluxnet2015,col=flag))+
geom_point()+
xlim(0,150)+
geom_hline(yintercept = 0,col="blue",lty=2)+
# scale_color_viridis_b()+
facet_wrap(.~sitename,nrow = 2)
#save the plot
# save.path<-"./test/check_reason_Tsoil_never_below_0degree/Figures/"
# htmlwidgets::saveWidget(
#   widget = plot1_dynamic, #the plotly object
#   file = paste0(save.path,"Tsoil_general_distributon.html"), #the path & file name
#   selfcontained = TRUE #creates a single html file
# )
#plot 2:
plot2<-df.merge_soil %>%
mutate(doy=yday(date))%>%
group_by(sitename)%>%
ggplot(aes(x=doy,y=TS_1_fluxnet2015,col=flag))+
geom_point()+
xlim(0,200)+
geom_hline(yintercept = 0,col="blue",lty=2)+
# scale_color_viridis_b()+
facet_wrap(.~sitename,nrow = 2)
plot2_dynamic<-ggplotly(plot2)
htmlwidgets::saveWidget(
widget = plot2_dynamic, #the plotly object
file = paste0(save.path,"Tsoil_vary_doy_eachsite.html"), #the path & file name
selfcontained = TRUE #creates a single html file
)
